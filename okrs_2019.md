# SourceCred 2019 OKRs

This document contains SourceCred's objectives, initiatives, and metrics for
2019, generated as part of Protocol Labs's annual planning process.
For an overview of SourceCred itself, see [the overview](overview.md).

The purpose of this document is to provide clarity around what the project's
goals and priorities are, and accountability to see if we are making effective
progress towards achieving those goals. We hope this will enable you to jump
into SourceCred with a clear sense of how the work we're prioritizing
day-to-day fits into our larger vision.

This document is not intended to lock us in to particular initiatives, or
metrics for success. SourceCred is less than a year old, and our opportunity
surface is quickly evolving. Also, our metrics have known deficiencies (e.g. we
don't have great ways to measure SourcCred's attribution quality).

As such, this is intended to be a living document. We will update it both to
record progress on our goals, and to modify and retarget our goals as we gain
more information. Since this document is in Git, all revisions will be
publically visible. If we remove or replace a goal, initiave, or metric, we'll
clearly explain the rationale for doing so in the associated commit.

# SourceCred Objectives

## 1. Improve Attribution Quality

SourceCred's core function is to produce cred attributions for open-source
projects. Improving the quality of those attributions is the our highest
priority. We want SourceCred to do a great job of assigning cred both to
contributors, and also to specific contributions.

SourceCred's current implementation is a promising start, but still leaves much
to be desired. It does a decent job of assigning cred to contributors; however,
it struggles with the greater challenge of valuing specific contributions. It
often assigns too much cred to [bikesheds] that have lots of activity but
produce little value. Also, SourceCred entirely misses contributions that don't
occur on GitHub; for example, if a project handles user support in a Slack, or
writes design docs on Google Docs, those contributions recieve no cred.

[bikesheds]: https://en.wikipedia.org/wiki/Law_of_triviality

We're not discouraged by these issues; credit attribution is a hard problem,
and we're just getting started! In 2019, improving the quality of these
attributions will be our top priority. Fortunately, we have a lot of ideas on
how to move forward. Broadly, they fall into two related categories: improving
the algorithm, and improving the data.

### Initiatives

#### Algorithm Improvements
- * [ ] Use temporal data in cred analysis
  - * [ ] Decide how to represent time (on edges? on nodes?)
  - * [ ] Decide semantics for time-weighted cred
  - * [ ] Enables asking "who earned cred this month"?
  - * [ ] Enables setting explicit weights over different time periods
- * [ ] Add support for heuristics
  - * [ ] Define elegant semantics for composition
  - * [ ] Ensure cohesion with other features (e.g. pass-thru semantics)
- * [ ] Add 'pass-thru' node semantics ([sourcecred/notes#1])
- * [ ] Add identity resolution
- * [ ] Add cred decomposition (who deserves the cred for a specific contribution?)

#### Data Improvements
- * [ ] Add file-level cred analysis
- * [ ] Add function call graph cred analysis
- * [ ] Enable communities to manually add nodes to the graph
- * [ ] Add heuristics to recognize project stewardship tasks like:
  - * [ ] writing documentation
  - * [ ] writing tests
  - * [ ] triaging issues

These initiatives involve a lot of subtle research and design work. Adding
heuristics requires designing an algorithm for composing heuristics with good
mathematical properties, and adding file-level cred means defining an approach
for consistently representing the identity of an individual file, and keeping
track of relationships across file renames, splits, and merges.

For these problems, we intend to first write very clear descriptions of the
problem, relevant considerations, and edge cases. Then, as we progress towards
a solution, we will document the design rationale as well as alternatives
considered.

#### Gaming Resistance
Also, as SourceCred sees more usage, we expect that "gaming" will be an increasing
problem, as participants try to manipulate the cred graph to inflate their own scores.
We should document our approach to resisting gaming, explore common attacks,
and persuasively demonstrate that we can mitigate them.

[sourcecred/notes#1]: https://github.com/sourcecred/notes/issues/1

### Metrics

Coming up with metrics for attribution quality is challenging, since there
isn't a readily available ground truth to compare against. Right now, the best
metric we have is asking the contributors to a project to rate the quality of
our attribution. The "cred rating" in the table below is the average rating
that contributors to a project give its cred attribution, measured on a
10-point scale.

| Metric                                  | Goal | Q4 | Q1 | Q2 | Q3 | Q4 |
| --------------------------------------- | ---- | -- | -- | -- | -- | -- |
| SourceCred's cred rating                | 8/10 | -  |    |    |    |    |
| IPFS's cred rating                      | 7/10 | -  |    |    |    |    |

We have a higher goal for SourceCred's average rating for two reasons:
- As a smaller and younger project, attributing its cred is simply easier
- We think that having a highly engaged community will enable producing better
  attributions, and we expect the SourceCred community to be more engaged

We think of cred ratings as a "least-bad" approach for measuring progress.
As we come up with better metrics, we will update this doc to include them.

## 2. Grow the SourceCred Community

We envision SourceCred being a thriving open-source community which is
passionate about recognizing and rewarding open-source contributors, about the
art of software engineering itself, and, of course, about building SourceCred.

Right now we have a pretty good start in that the founders are passionate about
these things ;) but we hope to grow the engaged community of contributors and
users.

On the initiative side, we want to improve the documentation around all parts of
SourceCred so that the project is easier to get up to speed on. We also intend
to produce content explaining SourceCred, like blog posts and video talks.

### Initiatives
- * [ ] Improve SourceCred's documentation
  - * [ ] Document the overall vision, mission, and philosophy
  - * [ ] Document the overall technical architecture
  - * [ ] Document the plugin system, and how to add a new one
- * [ ] Produce 2 engaging blog posts quarterly
- * [ ] Produce 1 engaging piece of audio/visual content quarterly

### Metrics

| Metric                                  | Goal | Q4 | Q1 | Q2 | Q3 | Q4 |
| --------------------------------------- | ---- | -- | -- | -- | -- | -- |
| # of core team members                  | 3    | 1  |    |    |    |    |
| # of unique code committers             | 30   | 3  |    |    |    |    |
| # of Discord community members          | 500  | 61 |    |    |    |    |

These metrics mostly record the number of people who have engaged with the community,
rather than its health and vibrancy. As such, they can be used as a general directional
proxy, but shouldn't be taken too seriously. The good news is, if the community is
thriving, it should be obvious. :)

## 3. Get Adoption of SourceCred

Right now, SourceCred is a prototype in search of some real users. Actual users
will be they only real source of feedback on the quality of our attributions.
Also, usage will bring more engaged and interested people into our community.

### Initiatives

We are planning an integration with [Open Collective]. We'll embed SourceCred
results into every open-source collectiveâ€™s page (e.g. here is the [page for
Webpack]). This way, collectives can thank their leading contributors as well
as their financial sponsors. We have buy-in from the Open Collective team, and
think this could be the start of a great partnership, in addition to bringing
more attention and users to SourceCred.

To do this, we need to make a number of engineering improvements to SourceCred.
The mirror module (which loads data from GitHub) needs to be made more robust
to errors in GitHub's GraphQL implementation ([#996]). We also need a proper
API for generating cred results, and support for running SourceCred on full
projects, not just individual repositories. All of this work is vital for any
SourceCred integrations, rather than being specific to the Open Collective
integration in particular.

[Open Collective]: https://opencollective.com/
[page for Webpack]: https://opencollective.com/webpack#contributors

- [ ] Integrate with Open Collective ([#935])
  - [ ] API-ify SourceCred ([#945], [#967])
  - [ ] Implement 'Project Mode' ([#701])
  - [ ] Make the SourceCred mirror module robust ([#998])

[#935]: https://github.com/sourcecred/sourcecred/issues/935
[#967]: https://github.com/sourcecred/sourcecred/issues/967
[#945]: https://github.com/sourcecred/sourcecred/issues/945
[#701]: https://github.com/sourcecred/sourcecred/issues/701
[#996]: https://github.com/sourcecred/sourcecred/issues/996
[#998]: https://github.com/sourcecred/sourcecred/issues/998


### Metrics

We aren't sure what the right metrics are to track engagement and usage by
downstream projects. For now, we'll measure "engagment", which is defined as:
the project has taken some externally visible action that shows they are using
SourceCred, e.g. by linking to the cred attribution in their README, adding
cred configuration files to their repository, or integrating a cred bot on
GitHub. Actions that can happen without anyone on the project taking direct action
(e.g. listing their cred on Open Collective) don't count.

For the purposes of the metrics below, "medium-sized projects" have at least 30 users
in their cred attribution, and "large-sized projects" have at least 100.

We'd like to see crypto projects start to use SourceCred as a way to transparently
distribute tokens to contributors. For purposes of the metric, we're tracking public
announcements or active distributions by well-known projects (i.e. no unknown ICOs).

Finally, we're interested in seeing third-party integrations, of which the Open Collective
integration is one.

| Metric                                  | Goal | Q4 | Q1 | Q2 | Q3 | Q4 |
| --------------------------------------- | ---- | -- | -- | -- | -- | -- |
| # of engaged medium-sized projects      | 50   | 0  |    |    |    |    |
| # of engaged larged-sized projects      | 5    | 0  |    |    |    |    |
| # of announced token distributions      | 3    | 0  |    |    |    |    |
| # of 3rd party integrations             | 3    | 0  |    |    |    |    |

## 4. Demonstrate that SourceCred Helps Projects

It's great if SourceCred has users. It's even better if we can show that we're
actively helping our users. SourceCred has the potential to drive positive
change in the communities using the technology, by fostering a greater sense of
appreciation for contributors, and making contributing more fun and engaging.
We'd like to be able to demonstrate that that SourceCred is helping projects in
some of the following ways:

- Engagement from new contributors
- Project test coverage
- Project documentation coverage
- Speed with which issues get triaged

### Initiatives

- * [ ] Make cred scores salient and engaging to contributors
  (may involve browser extensions or GitHub bots)
- * [ ] Measure how contributor behavior is different on projects using SourceCred
- * [ ] Iterate & demonstrate that SourceCred drives positive changes

### Metrics

The metrics for this goal are unclear. If we're successful, we should be able
to do e.g. a quantitative analysis showing that projects have an increase in
new contributor engagement after setting up SourceCred.
